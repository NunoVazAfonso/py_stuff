{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting\n",
    "-------\n",
    "\n",
    "Two different data pipelines exist -  a new one and an old one. \n",
    "\n",
    "The new pipeline handles a new, but very similiar data source. \n",
    "The old data pipeline is, in particular, limited in terms of effort needed to extend the data schema. it strictly enforces the schema, dropping non-complying records. \n",
    "However, several downstream applications rely on the old pipeline's output and its schema. \n",
    "The data schema in the new pipeline allows for more variation, and its necessary infrastructure described in code - deploying a new instance of it is simple. \n",
    "\n",
    "In both pipelines, the data goes through kafka as json strings, and is then stored in cassandra to later do lookups on visitor tokens.\n",
    "Find json examples for both schemas below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuno Afonso - Answers\n",
    "\n",
    "#### 0.   \n",
    "\n",
    "For this exercise, apart from my own experience and knowledge on these subjects, I will use some references, such as the book \"Cassandra: The Definitive Guide\", to help me address the presented problems in the most adequate way according to the technologies that are being used  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.   What partitioning and clustering keys would you consider for storing data in cassandra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to understand the relevant keys and attributes for storing data, we must first identify the business scenario, the available information and what are we looking to solve for the business. \n",
    "\n",
    "The information provided allows us to identify that:  \n",
    "(1) There are already pipelines that depend on the data that we are given, with a defined structure, so we'll have to maintain those  \n",
    "(2) The pipeline will provide a richer, more flexible data structure  \n",
    "(3) The data goes through the pipeline in the JSON format, and is stored in Cassandra with the goal of looking up on **visitor** tokens  \n",
    "  \n",
    "Given this information, and according to our third statement, the key that I would first consider as determinant for the business as **Partitioning Key**  would be the (a) **visitorToken** .  \n",
    "This is explicitly defined in our business scenario as the main key for our lookup, and therefore necessary for our applications to satisfy our business needs.  \n",
    "In this scenario, *clustering keys* could involve the *timestamp* of the event, the *asset* used to access our link or the *artists* of interest of our visitors.\n",
    "  \n",
    "  \n",
    "**Other Scenarios** for these could include :\n",
    "  \n",
    "Table: **Assets_by_visitor**  \n",
    "Goal: assessing which are the most used assets by visitors to access our content  \n",
    "Partition Key: visitorToken    \n",
    "Cluster Key: *asset*, *timestamp*  \n",
    "\n",
    "Table: **Artists_by_visitor**   \n",
    "Goal: assessing which are artists a visitor is most interested in  \n",
    "PK: visitorToken  \n",
    "CK: *artists*, *timestamp*  \n",
    "Secondary index: *artists* could also be marked as secondary index, for querying purposes   \n",
    "\n",
    "\n",
    "Other candidates for partitioning keys could also be the (b) **artists** or even the (c) **asset** attributes. As an example, we could have :  \n",
    "\n",
    "Table: **Visitors_by_artist**  \n",
    "Goal: evaluate the volume of visits by artists   \n",
    "PK: *artists*  \n",
    "CK: *visitorToken*, *timestamp*  \n",
    "\n",
    "Table: **Events_by_asset**  \n",
    "Goal: evaluate the most effective digital assets on our pages, that are having the most marketing impact  \n",
    "PK: *Asset*  \n",
    "CK: *event*, *timestamp*  \n",
    "  \n",
    "  \n",
    "In conclusion, the definition of our tables and queries would also highly depend on our business needs, but we proposed here a handfull of scenarios, which could constitute the starting point for analytical explorations and application usage and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Line out a strategy to get rid of the old setup while keeping downstream applications running uninterrupted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our objective os replacing the current schema on our data pipeline for a more flexible one, we now in anticipation that we will also have to guarantee the compatibility of the new schema with legacy applications.  \n",
    "In this way, we would need first to conduct efforts to:  \n",
    "\n",
    "(1) Identify and list all applications and services that are dependent on the old data structure  \n",
    "(2) Determine the criticality of downstream systems and the possibility of adaptation to our new schema  \n",
    "(3) Define a transition period, in which the responsible team would conduct the necessary communications and provide the guidelines for the migration for the new system  \n",
    "(4) Maintain both systems / structures during a transition period  \n",
    "(5) Discontinue the old service  \n",
    "  \n",
    "By looking at the data samples provided on this exercise we rapidly identify common patterns and similarities between the data, with the possibility of mapping values between the two schemas. In fact, the attributes *visitorToken*, *timestamp*, *timeuuid* and *event* are at the same level in the JSON object, and are in the same path.  \n",
    "However, the **old schema** provides a flat structure, with the only particularity of the attribute *artists* being an array.  \n",
    "On the other hand, the **new schema** reflects a richer structure, with nested attributes within the object, particularly in the *message* and *assetMetadata* attributes. These contain information, that was previously available in the first and only level of the old structure.  \n",
    "  \n",
    "That said, my approach would be to build a mapping module or tool for the new setup, which could grant compatibility with the old systems, by replicating exactly the same structure we were providing before. As we are instructed, the old schema needs to be \"strictly enforced\", but we have all the elements to do so in the new schema, by mapping attributes to the old structure.  \n",
    "  \n",
    "With this in mind, we can get rid of the old setup, by simply providing a mapping module to be integrated in the new pipeline, so that the same data structure keeps flowing and is respected for downstream applications, which strictly depend on it.  \n",
    "This pipeline can be maintained and made available solely for these applications, and be discontinued within a timeframe negotiated with the business and application owners.    \n",
    "New applications will from the moment of go-live on the other hand rely on the newest and more flexible schema, guaranteeing in this way the usage and maintenance of only one pipeline for both new and old applications.  \n",
    "  \n",
    "One example where this often happens is in API services, where when new data structures and objects emerge new endpoints might be needed and created, to support new objects or structures. In these scenarios, legacy API endpoints don't just simply disappear, but can be discontinued. Migrating for a new API endpoint, with a different structure is in these cases necessary or even enforced, usually within a negotiated timeframe.    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. For most strategies, it would be necessary to map data from one schema to the other for a transition period. Check out the python module below that forwards records from a kafka topic to another (execution and configuration is handled outside), and modify it to map data from one format to the other (pick one direction) based on the two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on I will try to address technically the mapping issue that we described earlier, on question 2.  \n",
    "On this point, we discussed that the best approach would be to **map the new to the old data structure**, in order to guarantee compatibility for a certain period of time.  \n",
    "  \n",
    "In order to be able to inspect and use the data we are given, we can start by defining the given JSON object as variables in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data variables\n",
    "\n",
    "old_schema_data = {\n",
    "  \"visitorToken\": \"b3cb81750c9689e4f7a2c85413b72c37\",\n",
    "  \"timestamp\": \"2020-06-23T10:38:25.843591Z\",\n",
    "  \"timeuuid\": \"ab3824b4-b53d-11ea-88af-0242ac120003\",\n",
    "  \"event\": \"visit\",\n",
    "  \"asset\": \"button\",\n",
    "  \"pipeline\": \"Waterslide\",\n",
    "  \"artists\": [\"Ariana Grande\"],\n",
    "  \"linkId\": \"efbbd33a-197c-45af-a0dc-9d85e012ce99\"\n",
    "}\n",
    "\n",
    "new_schema_data = {\n",
    "  \"visitorToken\": \"b3cb81750c9689e4f7a2c85413b72c37\",\n",
    "  \"timestamp\": \"2020-06-23T10:38:25.843591Z\",\n",
    "  \"timeuuid\": \"ab3824b4-b53d-11ea-88af-0242ac120003\",\n",
    "  \"event\": \"visit\",\n",
    "  \"schema\": \"visit-schema-1234\",\n",
    "  \"message\":{\n",
    "    \"asset\": \"button\",\n",
    "    \"assetversion\": \"1.0.0\",\n",
    "    \"pipeline\": \"Waterslide\",\n",
    "    \"assetMetadata\": {\n",
    "      \"artists\": [\"Ariana Grande\"]\n",
    "    },\n",
    "    \"linkId\": \"efbbd33a-197c-45af-a0dc-9d85e012ce99\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should also import the json library, to read and modify our data object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have the remaining of our Python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import structlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the remaining of the Python code (below), we can see that we have a couple of local dependencies from Kafka and our local project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "from . import base # base \n",
    "from ..connections import kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Kafka2Kafka class, which we use to instantiate our object and is the main component of the module. It initiates by logging and instantiating the Kafka reader and writer, passing the configurations previously loaded.  \n",
    "It also provides a *close* function, to terminate open Kafka resources and a *process_next* function, to process subsequent rows in our data pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kafka2Kafka(base.Sink):\n",
    "    \"\"\"Siphon data completely unmolested from kafka to kafka.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger) -> None:\n",
    "        \"\"\"Instantiate.\"\"\"\n",
    "        logger.debug(\"Instantiating Kafka Reader\")\n",
    "        self.reader = kafka.Reader(\n",
    "            config=config, logger=logger.getChild(\"kafka_reader\")\n",
    "        )\n",
    "        logger.debug(\"Instantiating Kafka Writer\")\n",
    "        self.writer = kafka.Writer(config, logger.getChild(\"writer\"))\n",
    "        self.logger = logger\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close open resources.\"\"\"\n",
    "        self.writer.close()\n",
    "\n",
    "    def process_next(self):\n",
    "        \"\"\"Atomically process the next record in line.\"\"\"\n",
    "        data = self.reader.read()\n",
    " \n",
    "\n",
    "        self.writer.write(data)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Kafka2Kafka.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to modify the *process_next* function to map the correct attributes to the old data schema. We will also need to include the *process_next* and the *close* functions within the *init* method, for these to be executed when the code runs.  \n",
    "That said, we can start by defining a mapping function, based on the assumptions that  \n",
    "(1) we will receive JSON objects in the data stream and  \n",
    "(2) the JSON objects on the new schema will have a \"*message*\" attribute  \n",
    "  \n",
    "In this way, we can first try to identify the type of object we are receiving. If it contains a *message* attribute, then we are dealing with a new schema object, and we will need to map it tou our old structure:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function. Returns data according to legacy data schema\n",
    "\n",
    "def map_to_legacy(data) :\n",
    "    if 'message' in data :\n",
    "        mapped_data = { \n",
    "            \"visitorToken\": data['visitorToken'],\n",
    "            \"timestamp\": data['timestamp'],\n",
    "            \"timeuuid\": data['timeuuid'],\n",
    "            \"event\": data['event'],\n",
    "            \"asset\": data['message']['asset'],\n",
    "            \"pipeline\": data['message']['pipeline'],\n",
    "            \"artists\": data['message']['assetMetadata']['artists'],\n",
    "            \"linkId\": data['message']['linkId']\n",
    "        }        \n",
    "        \n",
    "        return mapped_data\n",
    "    else: \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined our mapping function, we can execute it against our previously defined data variables, to test its results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test old schema data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"visitorToken\": \"b3cb81750c9689e4f7a2c85413b72c37\",\n",
      "    \"timestamp\": \"2020-06-23T10:38:25.843591Z\",\n",
      "    \"timeuuid\": \"ab3824b4-b53d-11ea-88af-0242ac120003\",\n",
      "    \"event\": \"visit\",\n",
      "    \"asset\": \"button\",\n",
      "    \"pipeline\": \"Waterslide\",\n",
      "    \"artists\": [\n",
      "        \"Ariana Grande\"\n",
      "    ],\n",
      "    \"linkId\": \"efbbd33a-197c-45af-a0dc-9d85e012ce99\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(map_to_legacy(old_schema_data), indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test new schema data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"visitorToken\": \"b3cb81750c9689e4f7a2c85413b72c37\",\n",
      "    \"timestamp\": \"2020-06-23T10:38:25.843591Z\",\n",
      "    \"timeuuid\": \"ab3824b4-b53d-11ea-88af-0242ac120003\",\n",
      "    \"event\": \"visit\",\n",
      "    \"asset\": \"button\",\n",
      "    \"pipeline\": \"Waterslide\",\n",
      "    \"artists\": [\n",
      "        \"Ariana Grande\"\n",
      "    ],\n",
      "    \"linkId\": \"efbbd33a-197c-45af-a0dc-9d85e012ce99\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(map_to_legacy(new_schema_data), indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing both examples, we can see that the returned object is the same, and is according to the original data structure, for our legacy data pipeline.  \n",
    "  \n",
    "Therefore, this function can be included in our code, and can be called within the *process_next* function, before the data is written in our destination table.  \n",
    "\n",
    "Lastly, we need to execute the methods that will enable us to process our data and define the their order of execution. In this sense, we will include the *process_next* and *close* functions in our *init* method, so they are triggered when we instantiate and run our object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kafka2Kafka** class, should finally look like this : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kafka2Kafka(base.Sink):\n",
    "    \"\"\"Siphon data completely unmolested from kafka to kafka.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger) -> None:\n",
    "        \"\"\"Instantiate.\"\"\"\n",
    "        logger.debug(\"Instantiating Kafka Reader\")\n",
    "        self.reader = kafka.Reader(\n",
    "            config=config, logger=logger.getChild(\"kafka_reader\")\n",
    "        )\n",
    "        logger.debug(\"Instantiating Kafka Writer\")\n",
    "        self.writer = kafka.Writer(config, logger.getChild(\"writer\"))\n",
    "        self.logger = logger\n",
    "        \n",
    "        self.process_next()    \n",
    "        \n",
    "        self.close()\n",
    "        \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close open resources.\"\"\"\n",
    "        self.writer.close()\n",
    "\n",
    "    def map_to_legacy(data) :\n",
    "        if 'message' in data :\n",
    "            mapped_data = { \n",
    "                \"visitorToken\": data['visitorToken'],\n",
    "                \"timestamp\": data['timestamp'],\n",
    "                \"timeuuid\": data['timeuuid'],\n",
    "                \"event\": data['event'],\n",
    "                \"asset\": data['message']['asset'],\n",
    "                \"pipeline\": data['message']['pipeline'],\n",
    "                \"artists\": data['message']['assetMetadata']['artists'],\n",
    "                \"linkId\": data['message']['linkId']\n",
    "            }        \n",
    "\n",
    "            return mapped_data\n",
    "        else: \n",
    "            return data    \n",
    "        \n",
    "    def process_next(self):\n",
    "        \"\"\"Atomically process the next record in line.\"\"\"\n",
    "        data = self.reader.read()\n",
    "         \n",
    "        data = map_to_legacy( data )\n",
    "\n",
    "        self.writer.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On question 3, a simple mapping approach was taken to try to guarantee in abstract terms the permanence of the JSON object structure. However, we can also try to handle other concerns, such as trying to handle exceptions in the new function or wrapping the process_next function in a while loop, to guarantee every row in our pipeline is processed and depending on the format we are getting our data, such as:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data: \n",
    "    data = map_to_legacy( d )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this field however, a broader understanding of the whole process and of Kafka itself was needed. In this sense, I tried to take a simplified approach that hopefully ilustrates a comprehensive solution for this problem, which could be further discussed in terms exception handling, data structure assurance and other technicalities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
